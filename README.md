# Awesome Instruction Following Papers

## Method


* [![arXiv](https://img.shields.io/badge/arXiv-2509.07414-red)](https://arxiv.org/abs/2509.07414) Language Self-Play For Data-Free Training
* [![arXiv](https://img.shields.io/badge/arXiv-2508.18255-red)](https://arxiv.org/abs/2508.18255) Hermes 4 Technical Report
* [![arXiv](https://img.shields.io/badge/arXiv-2506.23235-red)](https://arxiv.org/abs/2506.23235) Generalist Reward Models: Found Inside Large Language Models
* [![arXiv](https://img.shields.io/badge/arXiv-2506.09942-red)](https://arxiv.org/abs/2506.09942) VERIF: Verification Engineering for Reinforcement Learning in Instruction Following
* [![arXiv](https://img.shields.io/badge/arXiv-2506.01413-red)](https://arxiv.org/abs/2506.01413) Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models
* [![arXiv](https://img.shields.io/badge/arXiv-2505.19241-red)](https://arxiv.org/abs/2505.19241) ActiveDPO: Active Direct Preference Optimization for Sample-Efficient Alignment
* [![arXiv](https://img.shields.io/badge/arXiv-2502.00814-red)](https://arxiv.org/abs/2502.00814) Disentangling Length Bias in Preference Learning via Response-Conditioned Modeling
* [![arXiv](https://img.shields.io/badge/arXiv-2410.09584-red)](https://arxiv.org/abs/2410.09584) Toward General Instruction-Following Alignment for Retrieval-Augmented Generation
* [![arXiv](https://img.shields.io/badge/arXiv-2409.20370-red)](https://arxiv.org/abs/2409.20370) The Perfect Blend: Redefining RLHF with Mixture of Judges
* [![arXiv](https://img.shields.io/badge/arXiv-2409.14254-red)](https://arxiv.org/abs/2409.14254) Instruction Following without Instruction Tuning
* [![arXiv](https://img.shields.io/badge/arXiv-2408.15339-red)](https://arxiv.org/abs/2408.15339) UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized Implicit Reward Function
* [![arXiv](https://img.shields.io/badge/arXiv-2407.19594-red)](https://arxiv.org/abs/2407.19594) Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge
* [![arXiv](https://img.shields.io/badge/arXiv-2407.03502v1-red)](https://arxiv.org/abs/2407.03502v1) AgentInstruct: Toward Generative Teaching with Agentic Flows
* [![arXiv](https://img.shields.io/badge/arXiv-2406.16061-red)](https://arxiv.org/abs/2406.16061) PORT: Preference Optimization on Reasoning Traces
* [![arXiv](https://img.shields.io/badge/arXiv-2406.04412-red)](https://arxiv.org/abs/2406.04412) Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment
* [![arXiv](https://img.shields.io/badge/arXiv-2405.20830-red)](https://arxiv.org/abs/2405.20830) Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment
* [![arXiv](https://img.shields.io/badge/arXiv-2405.19874-red)](https://arxiv.org/abs/2405.19874) Is In-Context Learning Sufficient for Instruction Following in LLMs?
* [![arXiv](https://img.shields.io/badge/arXiv-2405.16455-red)](https://arxiv.org/abs/2405.16455) On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization
* [![arXiv](https://img.shields.io/badge/arXiv-2405.00675-red)](https://arxiv.org/abs/2405.00675) Self-Play Preference Optimization for Language Model Alignment
* [![arXiv](https://img.shields.io/badge/arXiv-2404.18922-red)](https://arxiv.org/abs/2404.18922) DPO Meets PPO: Reinforced Token Optimization for RLHF
* [![arXiv](https://img.shields.io/badge/arXiv-2404.13208-red)](https://arxiv.org/abs/2404.13208) The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions
* [![arXiv](https://img.shields.io/badge/arXiv-2404.10719-red)](https://arxiv.org/abs/2404.10719) Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study
* [![arXiv](https://img.shields.io/badge/arXiv-2403.07691-red)](https://arxiv.org/abs/2403.07691) ORPO: Monolithic Preference Optimization without Reference Model
* [![arXiv](https://img.shields.io/badge/arXiv-2402.10958-red)](https://arxiv.org/abs/2402.10958) Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts
* [![arXiv](https://img.shields.io/badge/arXiv-2401.01335-red)](https://arxiv.org/abs/2401.01335) Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models
* [![arXiv](https://img.shields.io/badge/arXiv-2401.18058-red)](https://arxiv.org/abs/2401.18058) LongAlign: A Recipe for Long Context Alignment of Large Language Models
* [![arXiv](https://img.shields.io/badge/arXiv-2310.04484-red)](https://arxiv.org/abs/2310.04484) Ada-Instruct: Adapting Instruction Generators for Complex Reasoning
* [![arXiv](https://img.shields.io/badge/arXiv-2305.18290-red)](https://arxiv.org/abs/2305.18290) Direct Preference Optimization: Your Language Model is Secretly a Reward Model
* [![arXiv](https://img.shields.io/badge/arXiv-2304.12244-red)](https://arxiv.org/abs/2304.12244) WizardLM: Empowering Large Pre-trained Language Models to Follow Complex Instructions
* [![arXiv](https://img.shields.io/badge/arXiv-2212.08073-red)](https://arxiv.org/abs/2212.08073) Constitutional AI: Harmlessness from AI Feedback

## Dataset

* [![arXiv](https://img.shields.io/badge/arXiv-2507.23751-red)](https://arxiv.org/abs/2507.23751) CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks
* [![arXiv](https://img.shields.io/badge/arXiv-2507.01352-red)](https://arxiv.org/abs/2507.01352) Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy
* [![arXiv](https://img.shields.io/badge/arXiv-2506.03968-red)](https://arxiv.org/abs/2506.03968) From REAL to SYNTHETIC: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding
* [![arXiv](https://img.shields.io/badge/arXiv-2505.23114-red)](https://arxiv.org/abs/2505.23114) Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data
* [![arXiv](https://img.shields.io/badge/arXiv-2502.18411-red)](https://arxiv.org/abs/2502.18411) OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference
* [![arXiv](https://img.shields.io/badge/arXiv-2412.11231-red)](https://arxiv.org/abs/2412.11231) Smaller Language Models Are Better Instruction Evolvers
* [![arXiv](https://img.shields.io/badge/arXiv-2406.17588-red)](https://arxiv.org/abs/2406.17588) LongIns: A Challenging Long-context Instruction-based Exam for LLMs
* [![arXiv](https://img.shields.io/badge/arXiv-2406.11191-red)](https://arxiv.org/abs/2406.11191) A Survey on Human Preference Learning for Large Language Models
* [![arXiv](https://img.shields.io/badge/arXiv-2307.04657-red)](https://arxiv.org/abs/2307.04657) BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset

## Evaluation

* [![arXiv](https://img.shields.io/badge/arXiv-2507.11538-red)](https://arxiv.org/abs/2507.11538) How Many Instructions Can LLMs Follow at Once?
* [![arXiv](https://img.shields.io/badge/arXiv-2502.14074-red)](https://arxiv.org/abs/2502.14074) Investigating Non-Transitivity in LLM-as-a-Judge
* [![arXiv](https://img.shields.io/badge/arXiv-2501.00560-red)](https://arxiv.org/abs/2501.00560) Re-evaluating Automatic LLM System Ranking for Alignment with Human Preference
* [![arXiv](https://img.shields.io/badge/arXiv-2412.01020-red)](https://arxiv.org/abs/2412.01020) AI Benchmarks and Datasets for LLM Evaluation
* [![arXiv](https://img.shields.io/badge/arXiv-2411.07037-red)](https://arxiv.org/abs/2411.07037) LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios
* [![arXiv](https://img.shields.io/badge/arXiv-2409.15268-red)](https://arxiv.org/abs/2409.15268) Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking
* [![arXiv](https://img.shields.io/badge/arXiv-2406.19314-red)](https://arxiv.org/abs/2406.19314) LiveBench: A Challenging, Contamination-Limited LLM Benchmark
* [![arXiv](https://img.shields.io/badge/arXiv-2406.12845-red)](https://arxiv.org/abs/2406.12845) Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts
* [![arXiv](https://img.shields.io/badge/arXiv-2403.16950-red)](https://arxiv.org/abs/2403.16950) Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators
* [![arXiv](https://img.shields.io/badge/arXiv-2403.13787-red)](https://arxiv.org/abs/2403.13787) RewardBench: Evaluating Reward Models for Language Modeling
* [![arXiv](https://img.shields.io/badge/arXiv-2402.13950-red)](https://arxiv.org/abs/2402.13950) Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning
* [![arXiv](https://img.shields.io/badge/arXiv-2401.06080-red)](https://arxiv.org/abs/2401.06080) Secrets of RLHF in Large Language Models Part II: Reward Modeling
* [![arXiv](https://img.shields.io/badge/arXiv-2401.03601-red)](https://arxiv.org/abs/2401.03601) InFoBench: Evaluating Instruction Following Ability in Large Language Models
* [![arXiv](https://img.shields.io/badge/arXiv-2310.07641-red)](https://arxiv.org/abs/2310.07641) Evaluating Large Language Models at Evaluating Instruction Following
* [![arXiv](https://img.shields.io/badge/arXiv-2307.04964-red)](https://arxiv.org/abs/2307.04964) Secrets of RLHF in Large Language Models Part I: PPO

## Analysis

* [![arXiv](https://img.shields.io/badge/arXiv-2503.10814-red)](https://arxiv.org/abs/2503.10814) Thinking Machines: A Survey of LLM based Reasoning Strategies
* [![arXiv](https://img.shields.io/badge/arXiv-2502.12568-red)](https://arxiv.org/abs/2502.12568) A Cognitive Writing Perspective for Constrained Long-Form Text Generation
* [![arXiv](https://img.shields.io/badge/arXiv-2505.14810-red)](https://arxiv.org/abs/2505.14810) Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models
* [![arXiv](https://img.shields.io/badge/arXiv-2505.11423-red)](https://arxiv.org/abs/2505.11423) When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs
* [![arXiv](https://img.shields.io/badge/arXiv-2504.11741-red)](https://arxiv.org/abs/2504.11741) Climbing the Ladder of Reasoning: What LLMs Can and Still Can't Solve after SFT?
* [![arXiv](https://img.shields.io/badge/arXiv-2504.02181-red)](https://arxiv.org/abs/2504.02181) A Survey of Scaling in Large Language Model Reasoning
* [![arXiv](https://img.shields.io/badge/arXiv-2410.14516-red)](https://arxiv.org/abs/2410.14516) Do LLMs "know" internally when they follow instructions?
* [![arXiv](https://img.shields.io/badge/arXiv-2410.07103-red)](https://arxiv.org/abs/2410.07103) Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context
* [![arXiv](https://img.shields.io/badge/arXiv-2407.11511-red)](https://arxiv.org/abs/2407.11511) Reasoning with Large Language Models, a Survey
* [![arXiv](https://img.shields.io/badge/arXiv-2404.05221-red)](https://arxiv.org/abs/2404.05221) LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models
* [![arXiv](https://img.shields.io/badge/arXiv-2403.18742-red)](https://arxiv.org/abs/2403.18742) Understanding the Learning Dynamics of Alignment with Human Feedback
* [![arXiv](https://img.shields.io/badge/arXiv-2402.15018-red)](https://arxiv.org/abs/2402.15018) Unintended Impacts of LLM Alignment on Global Representation

## Surveys

* [![arXiv](https://img.shields.io/badge/arXiv-2501.09686-red)](https://arxiv.org/abs/2501.09686) Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models
* [![arXiv](https://img.shields.io/badge/arXiv-2501.02189-red)](https://arxiv.org/abs/2501.02189) A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges
* [![arXiv](https://img.shields.io/badge/arXiv-2407.16216-red)](https://arxiv.org/abs/2407.16216) A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More
* [![arXiv](https://img.shields.io/badge/arXiv-2406.01252-red)](https://arxiv.org/abs/2406.01252) Towards Scalable Automated Alignment of LLMs: A Survey
* [![arXiv](https://img.shields.io/badge/arXiv-2307.12966-red)](https://arxiv.org/abs/2307.12966) Aligning Large Language Models with Human: A Survey
